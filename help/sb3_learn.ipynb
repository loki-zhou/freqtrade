{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "tmp_path = \"/tmp/sb3_log/\"\n",
    "# set up logger\n",
    "new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "\n",
    "model = A2C(\"MlpPolicy\", \"CartPole-v1\", verbose=1)\n",
    "# Set new logger\n",
    "model.set_logger(new_logger)\n",
    "model.learn(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "-----------------------------------------\n",
    "| eval/                   |             |         评估\n",
    "|    mean_ep_length       | 200         |          episode  平均长度\n",
    "|    mean_reward          | -157        |          episode  在评估期间的平均回报\n",
    "| rollout/                |             |\n",
    "|    ep_len_mean          | 200         |         平均回合长度（平均计算100个回合）\n",
    "|    ep_rew_mean          | -227        |         平均训练回合奖励\n",
    "| time/                   |             |\n",
    "|    fps                  | 972         |         每秒帧数（包括梯度更新所需的时间）\n",
    "|    iterations           | 19          |         总迭代次数（数据收集 + 策略更新，适用于 A2C/PPO 算法）\n",
    "|    time_elapsed         | 80          |         从训练开始以来经过的时间（秒数）\n",
    "|    total_timesteps      | 77824       |         总步数（即在环境中采取的行动次数）\n",
    "| train/                  |             |\n",
    "|    approx_kl            | 0.037781604 |         PPO 算法中旧策略和新策略之间的平均 KL 散度估计值。它是更新过程中变化幅度的一个近似值\n",
    "|    clip_fraction        | 0.243       |         PPO 算法中被剪辑的代理损失的平均比例（超过剪辑范围的阈值）。\n",
    "|    clip_range           | 0.2         |         PPO 算法中用于代理损失剪辑的当前剪辑因子值。\n",
    "|    entropy_loss         | -1.06       |         策略熵的平均值的负数。\n",
    "|    explained_variance   | 0.999       |         由值函数解释的回报方差分数\n",
    "|    learning_rate        | 0.001       |         学习率值\n",
    "|    loss                 | 0.245       |         当前总损失值\n",
    "|    n_updates            | 180         |         目前应用的梯度更新次数\n",
    "|    policy_gradient_loss | -0.00398    |         策略梯度损失的当前值（其值没有太多意义）。\n",
    "|    std                  | 0.205       |         使用广义状态相关探索（gSDE）时当前噪声的标准差。\n",
    "|    value_loss           | 0.226       | 用于在线策略算法的值函数损失的当前值。通常是值函数输出与蒙特卡洛估计（或 TD(lambda) 估计）之间的误差\n",
    "-----------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
